{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Reshape, Permute, merge, Flatten, concatenate\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "# import keras.layers\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils.layer_utils import convert_all_kernels_in_model\n",
    "# from imagenet_utils import decode_predictions\n",
    "#from keras.applications.vgg16 import preprocess_input, decode_prediction\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.applications import VGG16\n",
    "#from keras.layers import concatenate as concat\n",
    "\n",
    "# from imagenet_utils import preprocess_input\n",
    "from keras_applications.imagenet_utils import _obtain_input_shape\n",
    "#from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.optimizers import Adam\n",
    "# from __future__ import print_function\n",
    "# from __future__ import absolute_import\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from imutils import paths\n",
    "from keras import regularizers\n",
    "\n",
    "import cv2\n",
    "import keras.layers\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import os, sys\n",
    "# from imutils import paths\n",
    "import random\n",
    "import time\n",
    "import argparse\n",
    "import align.detect_face\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeXpression(include_top=True,\n",
    "                weights=None,\n",
    "                input_tensor=None,\n",
    "                input_shape=None,\n",
    "                padding=None,\n",
    "                classes=6):\n",
    "    # Check weights\n",
    "    if weights not in {'dexpression', None}:\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization) or `dexpression` '\n",
    "                         '(pre-training on ImageNet).')\n",
    "\n",
    "    if weights == 'imagenet' and include_top and classes != 6:\n",
    "        raise ValueError('If using `weights` as dexpression with `include_top`'\n",
    "                         ' as true, `classes` should be 6')\n",
    "    # Determine proper input shape\n",
    "    input_shape = _obtain_input_shape(\n",
    "        input_shape,\n",
    "        default_size=224,\n",
    "        min_size=139,\n",
    "        data_format=K.image_data_format(),\n",
    "        require_flatten=include_top)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        channel_axis = 1\n",
    "    else:\n",
    "        channel_axis = 3\n",
    "    # START MODEL\n",
    "    conv_1 = Convolution2D(64, (7, 7), strides=(2, 2), padding=padding, activation='relu', name='conv_1')(img_input)\n",
    "    maxpool_1 = MaxPooling2D((3, 3), strides=(2,2))(conv_1)\n",
    "    x = BatchNormalization()(maxpool_1)\n",
    "    \n",
    "    # FEAT-EX1\n",
    "    conv_2a = Convolution2D(96, (1, 1), strides=(1,1), activation='relu', padding=padding, name='conv_2a')(x)\n",
    "    conv_2b = Convolution2D(208, (3, 3), strides=(1,1), activation='relu', padding=padding, name='conv_2b')(conv_2a)\n",
    "    maxpool_2a = MaxPooling2D((3,3), strides=(1,1), padding=padding, name='maxpool_2a')(x)\n",
    "    conv_2c = Convolution2D(64, (1, 1), strides=(1,1), name='conv_2c')(maxpool_2a)\n",
    "    concat_1 = concatenate([conv_2b,conv_2c],axis=3,name='concat_2')\n",
    "    maxpool_2b = MaxPooling2D((3,3), strides=(1,1), padding=padding, name='maxpool_2b')(concat_1)\n",
    "    \n",
    "    # FEAT-EX2\n",
    "    conv_3a = Convolution2D(96, (1, 1), strides=(1,1), activation='relu', padding=padding, name='conv_3a')(maxpool_2b)\n",
    "    conv_3b = Convolution2D(208, (3, 3), strides=(1,1), activation='relu', padding=padding, name='conv_3b')(conv_3a)\n",
    "    maxpool_3a = MaxPooling2D((3,3), strides=(1,1), padding=padding, name='maxpool_3a')(maxpool_2b)\n",
    "    conv_3c = Convolution2D(64, (1, 1), strides=(1,1), name='conv_3c')(maxpool_2a)\n",
    "    concat_3 = concatenate([conv_3b,conv_3c],axis=3,name='concat_3')\n",
    "    maxpool_3b = MaxPooling2D((3,3), strides=(1,1), padding=padding, name='maxpool_3b')(concat_3)\n",
    "    \n",
    "    # FINAL LAYERS\n",
    "    net = Flatten()(maxpool_3b)\n",
    "    net = Dense(classes, activation='softmax', name='predictions')(net)\n",
    "    \n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "    # Create model.\n",
    "    model = Model(inputs, net, name='deXpression')\n",
    "    return model\n",
    "\n",
    "model= DeXpression(include_top=True,\n",
    "                weights=None,\n",
    "                input_tensor=None,\n",
    "                input_shape=None,\n",
    "                padding='same',\n",
    "                classes=6)\n",
    "\n",
    "model.load_weights('/media/topica/DATA/dexpress_kflold/best_result_26_10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "model_deep = model_from_json(open(\"/home/topica/workspace/Facial-Expression-Recognition/model_4layer_2_2_pool.json\", \"r\").read())\n",
    "model_deep.load_weights('/home/topica/workspace/Facial-Expression-Recognition/model_4layer_2_2_pool.h5') #load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ('angry', 'fear', 'happy', 'sad', 'surprise', 'neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ###When you want to dectect multi face\n",
    "image_size=160\n",
    "margin= 44\n",
    "gpu_memory_fraction=1.0\n",
    "detect_multiple_faces = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ###When you want to dectect multi face\n",
    "with tf.Graph().as_default():\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n",
    "    sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n",
    "    with sess.as_default():\n",
    "        pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None)\n",
    "frame = 0\n",
    "cap = cv2.VideoCapture(0)\n",
    "count = np.zeros((6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ###When you want to dectect multi face\n",
    "while(True):\n",
    "    ret, img = cap.read()\n",
    "    minsize = 20 # minimum size of face\n",
    "    threshold = [ 0.6, 0.7, 0.7 ]  # three steps's threshold\n",
    "    factor = 0.709 # scale factor\n",
    "    start_time = time.time()\n",
    "    bounding_boxes, _ = align.detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)\n",
    "    nrof_faces = bounding_boxes.shape[0]\n",
    "    if nrof_faces>0:\n",
    "        have_face = True\n",
    "        det = bounding_boxes[:,0:4]\n",
    "        det_arr = []\n",
    "        result_face = []\n",
    "        img_size = np.asarray(img.shape)[0:2]\n",
    "        if nrof_faces>1:\n",
    "            if detect_multiple_faces:\n",
    "                for i in range(nrof_faces):\n",
    "                    det_arr.append(np.squeeze(det[i]))\n",
    "            else:\n",
    "                bounding_box_size = (det[:,2]-det[:,0])*(det[:,3]-det[:,1])\n",
    "                img_center = img_size / 2\n",
    "                offsets = np.vstack([ (det[:,0]+det[:,2])/2-img_center[1], (det[:,1]+det[:,3])/2-img_center[0] ])\n",
    "                offset_dist_squared = np.sum(np.power(offsets,2.0),0)\n",
    "                index = np.argmax(bounding_box_size-offset_dist_squared*2.0) # some extra weight on the centering\n",
    "                det_arr.append(det[index,:])\n",
    "        else:\n",
    "            det_arr.append(np.squeeze(det))\n",
    "\n",
    "        for i, det in enumerate(det_arr):\n",
    "            det = np.squeeze(det)\n",
    "            bb = np.zeros(4, dtype=np.int32)\n",
    "            bb[0] = np.maximum(det[0]-margin/2, 0)\n",
    "            bb[1] = np.maximum(det[1]-margin/2, 0)\n",
    "            bb[2] = np.minimum(det[2]+margin/2, img_size[1])\n",
    "            bb[3] = np.minimum(det[3]+margin/2, img_size[0])\n",
    "            cropped = img[bb[1]:bb[3],bb[0]:bb[2],:]\n",
    "#             scaled = misc.imresize(cropped, (image_size, image_size), interp='bilinear')\n",
    "            cv2.rectangle(img,(bb[0],bb[1]),(bb[2],bb[3]),(164,64,64),2) #highlight detected face\n",
    "            #detected_face = cv2.cvtColor(cropped, cv2.COLOR_BGR2GRAY) #transform to gray scale\n",
    "            detected_face = cv2.resize(cropped, (224, 224)) #resize to 48x48\n",
    "#             print(detected_face.shape)\n",
    "            img_pixels = image.img_to_array(detected_face)\n",
    "            img_pixels = np.expand_dims(img_pixels, axis = 0)\n",
    "\n",
    "            img_pixels /= 255 #pixels are in scale of [0, 255]. normalize all pixels in scale of [0, 1]\n",
    "\n",
    "            #-----------------------------\n",
    "\n",
    "            predictions = model.predict(img_pixels) #store probabilities of 7 expressions\n",
    "            max_index = np.argmax(predictions[0])\n",
    "            ###deepcnn\n",
    "            detected_face_deep = cv2.cvtColor(cropped, cv2.COLOR_BGR2GRAY) #transform to gray scale\n",
    "            detected_face_deep = cv2.resize(detected_face_deep, (48, 48)) #resize to 48x48\n",
    "            img_pixels_deep = image.img_to_array(detected_face_deep)\n",
    "            img_pixels_deep = np.expand_dims(img_pixels_deep, axis = 0)\n",
    "\n",
    "            img_pixels_deep /= 255 #pixels are in scale of [0, 255]. normalize all pixels in scale of [0, 1]\n",
    "            predictions_deep = model_deep.predict(img_pixels_deep) #store probabilities of 7 expressions\n",
    "            max_index_deep = np.argmax(predictions_deep[0])\n",
    "            \n",
    "            \n",
    "            count[max_index] = count[max_index]+1\n",
    "            if (max_index_deep == 4):\n",
    "                result = \"%s %s%s\" % (emotions_deep[max_index_deep], round(predictions_deep[0][max_index_deep]*100, 2), '%')\n",
    "            elif (predictions[0][max_index] >= 0.75):\n",
    "                result = \"%s %s%s\" % (emotions[max_index], round(predictions[0][max_index]*100, 2), '%')\n",
    "            else :\n",
    "                result = \"neutral\"\n",
    "            \n",
    "            color = (55,55,255)\n",
    "            cv2.putText(img, result, (int(bb[0]+5), int(bb[1]-5)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1) \n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    cv2.imshow('img',img)\n",
    "    frame = frame + 1\n",
    "    if cv2.waitKey(70) & 0xFF == ord('q'): #press q to quit\n",
    "        break\n",
    "\n",
    "#kill open cv things\n",
    "# out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"the general emotion of the class: \" + emotions[np.argsort(count)[4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
